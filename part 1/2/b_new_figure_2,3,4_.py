# -*- coding: utf-8 -*-
"""prob_2_b_new_figure_2,3,4_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PqLOC2ILEN91UCA82FmfIRY2Z9_ka7TM
"""

import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)

class KernelPFF:
    def __init__(self, kernel_type="scalar", alpha=0.1):
        self.kernel_type = kernel_type
        self.alpha = alpha

    # Gaussian kernel
    def k(self, d2):
        return np.exp(-0.5 * d2 / self.alpha)

    # scalar kernel K(x,z) = k(||x - z||^2)
    def scalar_K(self, x, Z, cov):
        A = np.linalg.inv(cov)
        vals = []
        for z in Z:
            diff = x - z
            d2 = diff.T @ A @ diff
            vals.append(self.k(d2))
        return np.array(vals)

    # matrix kernel diag( k((x_i - z_i)^2 / sigma_i^2) )
    def matrix_K(self, x, Z, std):
        dim = len(x)
        K = np.zeros((dim, len(Z)))
        for a in range(dim):
            for j,z in enumerate(Z):
                d2 = (x[a] - z[a])**2 / (std[a]**2 + 1e-9)
                K[a,j] = self.k(d2)
        return K

    # gradient of log posterior (Gaussian prior + linear obs)
    def grad_log_post(self, x, mu, cov, y, obs_idx, R):
        dim = len(x)
        A = np.linalg.inv(cov)
        grad_prior = -A @ (x - mu)

        # likelihood gradient
        H = np.zeros((1,dim))
        H[0, obs_idx] = 1
        r = y - x[obs_idx]
        grad_like = H.T * (r / R)

        return grad_prior + grad_like.reshape(-1)

    # one flow step
    def flow(self, x, Z, mu, cov, y, obs_idx, R):
        dim = len(x)
        std = np.sqrt(np.diag(cov))
        grads = np.array([self.grad_log_post(z, mu, cov, y, obs_idx, R) for z in Z]).T

        if self.kernel_type == "scalar":
            K = self.scalar_K(x, Z, cov)
            w = K / (K.sum() + 1e-9)
            g = grads @ w
            # scalar kernel divergence term
            div = np.zeros(dim)
            A = np.linalg.inv(cov)
            for j,z in enumerate(Z):
                diff = x - z
                div += -K[j] * (A@diff) / self.alpha
            div /= len(Z)
        else:
            K = self.matrix_K(x, Z, std)
            g = np.zeros(dim)
            for a in range(dim):
                w = K[a] / (K[a].sum() + 1e-9)
                g[a] = grads[a] @ w
            # dimension-wise divergence
            div = np.zeros(dim)
            for a in range(dim):
                for j,z in enumerate(Z):
                    diff = x[a] - z[a]
                    div[a] += -K[a,j] * diff / (std[a]**2*self.alpha + 1e-9)
                div[a] /= len(Z)

        return cov @ (g - div)

def figure2(show_arrows=False):
    dim = 2
    n = 30

    # Similar rates case
    cov_A = np.array([[1,0],[0,1]])
    # Different rate case (2D anisotropic)
    cov_B = np.array([[1,0],[0,0.05]])

    mu = np.zeros(dim)
    obs_idx = 1
    R = 0.2**2
    y = 1.0  # posterior moves toward this horizontal line

    fig, axes = plt.subplots(2,2, figsize=(12,10))

    cases = [(cov_A, "scalar"), (cov_A, "matrix"),
             (cov_B, "scalar"), (cov_B, "matrix")]

    for k,(cov,kernel) in enumerate(cases):
        row = k//2
        col = k%2
        ax = axes[row][col]

        X = np.random.multivariate_normal(mu, cov, n)
        pff = KernelPFF(kernel_type=kernel, alpha=0.1)

        Z = X.copy()
        for step in range(100):
            X_new = X.copy()
            for i in range(n):
                X_new[i] = X[i] + 0.05*pff.flow(X[i], X, mu, cov, y, obs_idx, R)
            X = X_new

        ax.scatter(Z[:,0], Z[:,1], c='gray', label='prior')
        ax.scatter(X[:,0], X[:,1], c='red', label='posterior')
        ax.axhline(y=y, linestyle='--', color='green')
        ax.set_title(f"cov={'A' if row==0 else 'B'} , kernel={kernel}")
        ax.set_xlim(-3,3)
        ax.set_ylim(-3,3)
        ax.grid(True)

    plt.show()

def figure3():
    dim = 10
    n = 40

    cov = np.eye(dim)
    mu = np.zeros(dim)

    X0 = np.random.multivariate_normal(mu, cov, n)
    true_x = np.random.randn(dim)

    obs_idx = 2
    R = 0.1**2
    y = true_x[obs_idx] + np.random.normal(0,0.1)

    fig, axes = plt.subplots(1,2, figsize=(12,6))

    for j,kernel in enumerate(["scalar","matrix"]):
        pff = KernelPFF(kernel_type=kernel, alpha=0.1)
        X = X0.copy()
        for step in range(120):
            X_new = X.copy()
            for i in range(n):
                X_new[i] = X[i] + 0.05*pff.flow(X[i], X, mu, cov, y, obs_idx, R)
            X = X_new

        ax = axes[j]
        ax.scatter(X0[:,0], X0[:,1], c='gray', label='prior')
        ax.scatter(X[:,0],  X[:,1],  c='red', label='posterior')
        ax.scatter(true_x[0], true_x[1], c='blue', marker='x', s=120)

        ax.set_title(f"kernel={kernel}")
        ax.set_xlim(-3,3)
        ax.set_ylim(-3,3)
        ax.grid(True)

    plt.show()

figure2()
figure3()

import matplotlib.pyplot as plt
import matplotlib as mpl

mpl.rcParams['font.family'] = 'Times New Roman'
mpl.rcParams['font.size'] = 14
mpl.rcParams['axes.titlesize'] = 16
mpl.rcParams['axes.labelsize'] = 14
mpl.rcParams['xtick.labelsize'] = 12
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['legend.fontsize'] = 12
mpl.rcParams['figure.dpi'] = 120
mpl.rcParams['axes.linewidth'] = 1.0

import numpy as np
import matplotlib.pyplot as plt

# Using the KernelPFF class you defined earlier in Colab
# This reuses the figure2() logic but with improved layout/color/annotation

def figure2_exact():
    dim = 2
    n = 30

    cov_A = np.array([[1,0],[0,1]])
    cov_B = np.array([[1,0],[0,0.05]])

    mu = np.zeros(dim)
    obs_idx = 1
    R = 0.2**2
    y = 1.0

    fig, axes = plt.subplots(2, 2, figsize=(10,10))
    fig.subplots_adjust(hspace=0.25, wspace=0.15)

    cases = [
        (cov_A, "scalar", "(a) scalar kernel, equal rates"),
        (cov_A, "matrix", "(b) matrix kernel, equal rates"),
        (cov_B, "scalar", "(c) scalar kernel, unequal rates"),
        (cov_B, "matrix", "(d) matrix kernel, unequal rates")
    ]

    for k,(cov,kernel,label) in enumerate(cases):
        row = k//2
        col = k%2
        ax = axes[row][col]

        # sample prior particles
        X0 = np.random.multivariate_normal(mu, cov, n)

        # run PFF
        X = X0.copy()
        pff = KernelPFF(kernel_type=kernel, alpha=0.1)
        for step in range(100):
            newX = X.copy()
            for i in range(n):
                newX[i] = X[i] + 0.05*pff.flow(X[i], X, mu, cov, y, obs_idx, R)
            X = newX

        # draw prior & posterior
        ax.scatter(X0[:,0], X0[:,1],
                   c='gray', s=50, alpha=0.7, label="Prior")
        ax.scatter(X[:,0],  X[:,1],
                   c='red', s=50, alpha=0.9, label="Posterior")

        # observation line
        ax.axhline(y=y, linestyle='--', color='green', linewidth=1.2)

        # axis ranges (paper style)
        ax.set_xlim(-3,3)
        ax.set_ylim(-3,3)

        ax.grid(True, linestyle=':', linewidth=0.5, alpha=0.6)

        # subplot label
        ax.text(-2.9, 2.6, label, fontsize=15, fontweight='bold')

        if row==1:
            ax.set_xlabel("x₁ (unobserved)")
        if col==0:
            ax.set_ylabel("x₂ (observed)")

    plt.show()

def figure3_exact():
    dim = 10
    n = 40
    cov = np.eye(dim)
    mu = np.zeros(dim)

    X0 = np.random.multivariate_normal(mu, cov, n)
    true_x = np.random.randn(dim)

    obs_idx = 2
    R = 0.1**2
    y = true_x[obs_idx] + np.random.normal(0,0.1)

    fig, axes = plt.subplots(1,2, figsize=(12,5))
    fig.subplots_adjust(wspace=0.2)

    for j,kernel in enumerate(["scalar","matrix"]):
        pff = KernelPFF(kernel_type=kernel, alpha=0.1)
        X = X0.copy()

        for step in range(120):
            newX = X.copy()
            for i in range(n):
                newX[i] = X[i] + 0.05*pff.flow(
                    X[i], X, mu, cov, y, obs_idx, R
                )
            X = newX

        ax = axes[j]

        # prior
        ax.scatter(X0[:,0], X0[:,1], c='gray', s=55, alpha=0.7)

        # posterior
        ax.scatter(X[:,0], X[:,1], c='red', s=55, alpha=0.9)

        # truth
        ax.scatter(true_x[0], true_x[1], c='blue', marker='*', s=200)

        # 布局、颜色、字体完全论文风格
        ax.set_xlim(-3,3)
        ax.set_ylim(-3,3)
        ax.grid(True, linestyle=':', linewidth=0.5, alpha=0.6)

        if j==0:
            ax.set_ylabel("x₂")
        ax.set_xlabel("x₁")

        ax.set_title(
            f"({chr(ord('a')+j)}) {kernel} kernel",
            fontweight='bold'
        )

    plt.show()

!wget -O /usr/local/lib/python3.10/dist-packages/matplotlib/mpl-data/fonts/ttf/TimesNewRoman.ttf \
https://raw.githubusercontent.com/python-stock-dev/fonts/main/TimesNewRoman.ttf

!fc-cache -f -v

figure2_exact()
figure3_exact()

"""figure 4 not working"""

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

# ===============================================================
#  LORENZ-96 MODEL
# ===============================================================
def l96(x, F=8.0):
    """Lorenz-96 dynamics."""
    d = len(x)
    dx = np.zeros(d)
    for i in range(d):
        dx[i] = (x[(i+1)%d] - x[i-2]) * x[i-1] - x[i] + F
    return dx

def rk4_step(x, dt=0.01, F=8.0):
    """One RK4 step for Lorenz-96."""
    k1 = l96(x, F)
    k2 = l96(x + 0.5*dt*k1, F)
    k3 = l96(x + 0.5*dt*k2, F)
    k4 = l96(x + dt*k3, F)
    return x + dt*(k1 + 2*k2 + 2*k3 + k4)/6.0


# ===============================================================
#  PARTICLE FLOW FILTER (PFF)
# ===============================================================
class KernelPFF:
    """Matrix-kernel PFF for assimilating Lorenz-96."""

    def __init__(self, n_particles=50, alpha=0.05):
        self.n_particles = n_particles
        self.alpha = alpha  # kernel bandwidth

    def gaussian_kernel(self, d2):
        return np.exp(-0.5 * d2 / self.alpha)

    def matrix_kernel_value(self, x, particles, prior_std):
        dim = len(x)
        K = np.zeros((dim, self.n_particles))
        for a in range(dim):
            for i in range(self.n_particles):
                d2 = (x[a] - particles[i, a])**2 / (prior_std[a]**2 + 1e-8)
                K[a, i] = self.gaussian_kernel(d2)
        return K

    def grad_log_post(self, x, prior_mean, prior_cov, obs, obs_idx, R):
        dim = len(x)

        # prior gradient: ∇ log p_prior
        grad_prior = -np.linalg.solve(prior_cov + 1e-6*np.eye(dim), x - prior_mean)

        # likelihood for partial observation Hx = y
        H = np.zeros((len(obs_idx), dim))
        for i, idx in enumerate(obs_idx):
            H[i, idx] = 1.0
        innovation = obs - x[obs_idx]
        grad_like = H.T @ np.linalg.solve(R, innovation)
        return grad_prior + grad_like

    def divergence_matrix(self, x, particles, prior_std):
        dim = len(x)
        K = self.matrix_kernel_value(x, particles, prior_std)
        div = np.zeros(dim)
        for a in range(dim):
            for i in range(self.n_particles):
                diff = x[a] - particles[i, a]
                div[a] += -K[a, i] * diff / (prior_std[a]**2 * self.alpha + 1e-8)
        return div / self.n_particles

    def particle_flow(self, x, particles, prior_mean, prior_cov, obs, obs_idx, R):
        dim = len(x)
        prior_std = np.sqrt(np.diag(prior_cov))

        # gradients of all particles ∇ log p
        grads = np.zeros((dim, self.n_particles))
        for i in range(self.n_particles):
            grads[:, i] = self.grad_log_post(
                particles[i], prior_mean, prior_cov, obs, obs_idx, R
            )

        # matrix kernel weights
        K = self.matrix_kernel_value(x, particles, prior_std)
        weighted = np.zeros(dim)
        for a in range(dim):
            wa = K[a]
            weighted[a] = np.sum(wa * grads[a]) / (np.sum(wa) + 1e-10)

        div = self.divergence_matrix(x, particles, prior_std)
        return prior_cov @ (weighted - div)

    def update(self, particles, prior_mean, prior_cov, obs, obs_idx, R,
               n_iter=80, dt=0.03):
        X = particles.copy()
        for _ in range(n_iter):
            newX = X.copy()
            for i in range(self.n_particles):
                flow = self.particle_flow(
                    X[i], X, prior_mean, prior_cov, obs, obs_idx, R
                )
                newX[i] = X[i] + dt * flow
            X = newX
        return X


# ===============================================================
#  ENKF with inflation γ
# ===============================================================
def enkf_update(ensemble, obs, obs_idx, R, gamma=1.0):
    """
    Simple LETKF/EnKF-style update with multiplicative inflation γ.
    """
    N, dim = ensemble.shape
    X = ensemble.copy()

    # observation operator H
    H = np.zeros((len(obs_idx), dim))
    for i, idx in enumerate(obs_idx):
        H[i, idx] = 1.0

    # prior mean and perturbations
    mean = X.mean(axis=0)
    A = X - mean  # N×dim

    # multiplicative inflation
    A_infl = gamma * A
    X_infl = mean + A_infl

    # observation space perturbations
    Y = X_infl[:, obs_idx]
    y_mean = Y.mean(axis=0)
    Y_pert = Y - y_mean  # N×n_obs

    # covariances
    PfHt = (A_infl.T @ Y_pert) / (N - 1)              # dim×n_obs
    HPfHt = (Y_pert.T @ Y_pert) / (N - 1) + R         # n_obs×n_obs
    K = PfHt @ np.linalg.inv(HPfHt)                   # dim×n_obs

    innov = obs - Y                                   # N×n_obs
    X_a = X_infl + (innov @ K.T)                      # N×dim

    return X_a


# ===============================================================
#  RUN L96 EXPERIMENT (returns RMSE curves)
# ===============================================================
def run_experiment(dim=40, F=8.0, dt=0.01,
                   n_steps=400, obs_gap=5,
                   obs_idx=None,
                   n_particles=50,
                   gamma=1.0,
                   alpha=0.05):

    if obs_idx is None:
        obs_idx = np.arange(0, dim, 2)  # observe every second grid point

    # ---- generate truth trajectory ----
    truth = np.zeros((n_steps, dim))
    truth[0] = np.random.randn(dim)
    for t in range(1, n_steps):
        truth[t] = rk4_step(truth[t-1], dt, F=F)

    # ---- generate observations ----
    R = 0.2**2 * np.eye(len(obs_idx))
    obs_list = []
    for t in range(n_steps):
        if t % obs_gap == 0:
            obs = truth[t, obs_idx] + np.random.normal(0, 0.2, size=len(obs_idx))
        else:
            obs = None
        obs_list.append(obs)

    # ---- initialize ensembles ----
    P0 = np.eye(dim) * 1.0
    X_pff = np.random.multivariate_normal(truth[0], P0, n_particles)
    X_enkf = np.random.multivariate_normal(truth[0], P0, n_particles)

    pff = KernelPFF(n_particles=n_particles, alpha=alpha)

    rmse_pff = []
    rmse_enkf = []

    # ---- filtering loop ----
    for t in range(n_steps):

        # forecast step
        for i in range(n_particles):
            X_pff[i] = rk4_step(X_pff[i], dt, F=F)
            X_enkf[i] = rk4_step(X_enkf[i], dt, F=F)

        if obs_list[t] is not None:
            obs = obs_list[t]

            # EnKF update
            X_enkf = enkf_update(X_enkf, obs, obs_idx, R, gamma=gamma)

            # PFF update
            prior_mean = X_pff.mean(axis=0)
            prior_cov = np.cov(X_pff.T) + 1e-6*np.eye(dim)
            X_pff = pff.update(
                X_pff, prior_mean, prior_cov, obs, obs_idx, R,
                n_iter=80, dt=0.03
            )

        # compute RMSE (ensemble mean vs truth)
        mean_pff = X_pff.mean(axis=0)
        mean_enkf = X_enkf.mean(axis=0)

        rmse_pff.append(np.sqrt(np.mean((mean_pff - truth[t])**2)))
        rmse_enkf.append(np.sqrt(np.mean((mean_enkf - truth[t])**2)))

    return np.array(rmse_enkf), np.array(rmse_pff)


# ===============================================================
#  RUN TWO EXPERIMENTS FOR FIGURE 4 (four scenarios)
# ===============================================================
# 1) γ = 1, α = 0.05
rmse_letkf1, rmse_pff1 = run_experiment(gamma=1.0,  alpha=0.05)

# 2) γ = 1.25, α = 0.01
rmse_letkf125, rmse_pff125 = run_experiment(gamma=1.25, alpha=0.01)


# ===============================================================
#  PLOT FIGURE 4 (four subplots, real L96 RMSE)
# ===============================================================
T = np.arange(len(rmse_letkf1))

fig, axes = plt.subplots(2, 2, figsize=(12, 9))
fig.subplots_adjust(hspace=0.25, wspace=0.15)

panels = [
    (rmse_letkf1,   "(a) LETKF, γ=1"),
    (rmse_letkf125, "(b) LETKF, γ=1.25"),
    (rmse_pff1,     "(c) PFF, γ=1, α=0.05"),
    (rmse_pff125,   "(d) PFF, γ=1.25, α=0.01")
]

for k, (rmse_curve, title) in enumerate(panels):
    r = k // 2
    c = k % 2
    ax = axes[r][c]

    ax.plot(T, rmse_curve, color='black', linewidth=2)

    ax.set_yscale("log")
    ax.set_xlim(0, len(T)-1)
    ax.set_ylim(1e-2, 3)

    ax.set_title(title, fontsize=16, fontweight="bold")
    ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)

    if r == 1:
        ax.set_xlabel("Time step", fontsize=14)
    if c == 0:
        ax.set_ylabel("RMSE", fontsize=14)

plt.tight_layout()
plt.show()

print("Figure 4-style plot generated with REAL L96 experiment RMSE.")